import os
import re
import unicodedata

# --- ×”×’×“×¨×•×ª ×§×‘×¦×™× ---
FULL_INPUT_FILE = "aspects_transit.txt"
OUTPUT_FILE = "aspects_transit_output.txt"

FINAL_SORTED_FILE = "aspects_transit_FINAL_SORTED.txt"
ERROR_FILE = "aspects_transits_analysi_errors.txt"


# --- ×¤×•× ×§×¦×™×™×ª × ×•×¨××œ×™×–×¦×™×” (×›×“×™ ×œ×•×•×“× ×©×™×•×•×™×•×Ÿ ××œ×) ---
def ultimate_normalize_text(text):
    if not text:
        return ""
    text = unicodedata.normalize('NFC', text)
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)
    return text


# --- ×©×œ×‘ 1: ×‘× ×™×™×ª ××¤×ª × ×™×ª×•×— (Analysis Map) ---

def build_analysis_map_robust(output_filename, original_aspects_set):
    """
    ×§×•×¨× ××ª ×§×•×‘×¥ ×”×¤×œ×˜ ×‘×¦×•×¨×” ×—×¡×™× ×ª ×›×©×œ×™×, ×•××™×™×¦×¨ ××™×œ×•×Ÿ:
    {×›×•×ª×¨×ª ×× ×•×¨××œ×ª: ×”× ×™×ª×•×— ×”××œ×}.
    """
    analysis_map = {}

    try:
        with open(output_filename, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f]

        i = 0
        while i < len(lines):
            current_line = lines[i]
            # ğŸ’¡ ×˜×™×¤: ××•××œ×¥ ×ª××™×“ ×œ× ×§×•×ª ×©×•×¨×” ××ª×•×•×™ ×¨×•×•×— ××™×•×ª×¨×™× ×œ×¤× ×™ ×”×¤×™×¦×•×œ
            current_line_split = current_line.strip().split()

            if current_line_split and current_line_split[-1] == 'retrograde':
                # 1. ×©××™×¨×ª ×”××™×œ×” ×”×¨××©×•× ×” (×œ×“×•×’××”: '××¨×§×•×¨×™')
                first_word = current_line_split[0]

                # 2. ×©××™×¨×ª ×”××™×œ×™× ×”×××¦×¢×™×•×ª (××”×©× ×™×™×” ×¢×“ ×œ×¤× ×™ ×”××—×¨×•× ×”)
                middle_words = current_line_split[1:-1]

                # 3. ×‘× ×™×™×” ××—×“×© ×©×œ ×”×©×•×¨×” ×‘×××¦×¢×•×ª join, ×”×•×¡×¤×ª 'retrograde'
                #    ×‘××§×•× ×”×©× ×™, ×•×¦×™×¨×•×£ ×”××™×œ×™× ×”×××¦×¢×™×•×ª

                # ×‘× ×™×™×ª ×”×¨×©×™××” ×”×—×“×©×”: [first_word, 'retrograde', *middle_words]
                new_list = [first_word, 'retrograde'] + middle_words

                # ×—×™×‘×•×¨ ×”×¨×©×™××” ×œ××—×¨×•×–×ª ×©×•×¨×” ××—×ª
                current_line = " ".join(new_list)

            # 1. ×‘×“×™×§×”: ×”×× ×”×©×•×¨×” ×”× ×•×›×—×™×ª ×”×™× ×›×•×ª×¨×ª?
            normalized_current_line = ultimate_normalize_text(current_line)

            if normalized_current_line in original_aspects_set:
                # 2. ×× ×–×• ×›×•×ª×¨×ª, × × ×™×— ×©×”×©×•×¨×” ×”×‘××” ×”×™× ×”× ×™×ª×•×— ×©×œ×”
                if i + 1 < len(lines):
                    llm_analysis = lines[i + 1].strip()

                    # ×•×“× ×©×”× ×™×ª×•×— ××™× ×• ×›×•×ª×¨×ª ××• ×”×•×“×¢×ª ×©×’×™××”
                    if not llm_analysis.startswith('[×©×’×™××”:') and normalized_current_line != ultimate_normalize_text(
                            llm_analysis):
                        analysis_map[normalized_current_line] = llm_analysis
                        # ×§×•×¤×¥ ××¢×‘×¨ ×œ×›×•×ª×¨×ª ×•×œ× ×™×ª×•×—, ××—×¤×© ××ª ×”×›×•×ª×¨×ª ×”×‘××”
                        i += 2
                        continue

            # ×× ×œ× × ××¦××” ×›×•×ª×¨×ª/× ×™×ª×•×— ×ª×§×™× ×™×, ××§×“××™× ××ª ×”××•× ×” ×‘-1 ×•×××©×™×›×™×
            i += 1

    except FileNotFoundError:
        print(f"×©×’×™××”: ×§×•×‘×¥ ×”×¤×œ×˜ ×œ× × ××¦×: {output_filename}")
        return {}

    return analysis_map


# --- ×©×œ×‘ 2: ××™×•×Ÿ ×•×™×™×¦×•× ×¡×•×¤×™ ---

def sort_and_export():
    print("--- ××ª×—×™×œ ×©×œ×‘ 1: ×§×¨×™××ª ×¡×“×¨ ×”××§×•×¨ ---")

    # ×§×•×¨× ××ª ×›×œ ×”×›×•×ª×¨×•×ª ×”××§×•×¨×™×•×ª ×œ×¡×˜ ×•×œ×¨×©×™××”
    try:
        with open(FULL_INPUT_FILE, 'r', encoding='utf-8') as f:
            original_order_list = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print(f"×©×’×™××”: ×§×•×‘×¥ ×”××§×•×¨ {FULL_INPUT_FILE} ×œ× × ××¦×.")
        return

    # ×‘× ×™×™×ª ×¡×˜ ×”×›×•×ª×¨×•×ª ×”×× ×•×¨××œ×•×ª
    original_aspects_set = {ultimate_normalize_text(a) for a in original_order_list}

    print("--- ××ª×—×™×œ ×©×œ×‘ 2: ×‘× ×™×™×ª ××¤×ª ×”× ×™×ª×•×—×™× ×”×—×¡×™× ×” ---")
    analysis_map = build_analysis_map_robust(OUTPUT_FILE, original_aspects_set)

    if len(analysis_map) == 0:
        print("×©×’×™××” ×§×¨×™×˜×™×ª: ×œ× × ××¦××• × ×™×ª×•×—×™× ×‘××¤×ª ×”× ×™×ª×•×—. ×¡×™×•×.")
        return

    print(f"× ××¦××• {len(analysis_map):,} × ×™×ª×•×—×™× ×™×™×—×•×“×™×™× ×‘××¤×ª ×”× ×™×ª×•×—.")
    print("-" * 30)
    print("--- ××ª×—×™×œ ×©×œ×‘ 3: ×§×¨×™××ª ×¡×“×¨ ×”××§×•×¨ ×•×™×™×¦×•× ×××•×™×Ÿ ---")

    sorted_count = 0
    missing_in_map = 0
    error_log = []

    with open(FINAL_SORTED_FILE, 'w', encoding='utf-8') as f_final:

        # ×¢×•×‘×¨ ×¢×œ ×”×›×•×ª×¨×•×ª ×”××§×•×¨×™×•×ª ×œ×¤×™ ×”×¡×“×¨
        for raw_aspect in original_order_list:

            normalized_aspect = ultimate_normalize_text(raw_aspect)

            # ×‘×“×™×§×”: ×× ×”× ×™×ª×•×— ×§×™×™× ×‘××¤×”
            if normalized_aspect in analysis_map:
                llm_analysis = analysis_map[normalized_aspect]

                # ×›×ª×™×‘×ª ×”×›×•×ª×¨×ª ×”××§×•×¨×™×ª (raw) ×•×”× ×™×ª×•×— ×©×œ×” ×œ×§×•×‘×¥ ×”×××•×™×Ÿ
                f_final.write(f"{raw_aspect}\n")
                f_final.write(f"{llm_analysis}\n")
                f_final.write("\n")
                sorted_count += 1
            else:
                missing_in_map += 1
                error_log.append(raw_aspect)

    print("-" * 30)
    print(f"âœ… ×¡×™×•× ××™×•×Ÿ! × ×›×ª×‘×• {sorted_count:,} × ×™×ª×•×—×™× ×××•×™× ×™× ×œ×§×•×‘×¥: {FINAL_SORTED_FILE}")

    # ×‘×“×™×§×” ×¡×•×¤×™×ª (×× ×–×” ×œ× 0, ×–×” ×”×‘×¢×™×”!)
    if missing_in_map > 0:
        print(f"âŒ ××–×”×¨×” ×—××•×¨×”: {missing_in_map:,} ×›×•×ª×¨×•×ª ×œ× × ××¦××• ×‘××¤×ª ×”× ×™×ª×•×— ×”××œ××” (× ×©××¨×• ×œ-{ERROR_FILE}).")
        with open(ERROR_FILE, 'w', encoding='utf-8') as f_err:
            f_err.write('\n'.join(error_log) + '\n')


# --- ×”×¤×¢×œ×ª ×”×¡×§×¨×™×¤×˜ ---
if __name__ == "__main__":
    sort_and_export()
